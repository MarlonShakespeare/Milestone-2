{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "viral-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm \n",
    "import re\n",
    "from gensim.utils import simple_preprocess \n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intermediate-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataframe of labeled tweets\n",
    "\n",
    "tweet_df = pd.read_json('tweet.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cardiovascular-lesbian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17461978</td>\n",
       "      <td>[RT @CarnivalCruise: üéâ Are you ready to see wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1297437077403885568</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17685258</td>\n",
       "      <td>[RT @realDonaldTrump: THANK YOU #RNC2020! http...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15750898</td>\n",
       "      <td>[A family fears they may have been cheated out...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1659167666</td>\n",
       "      <td>[RT @VonteThePlug: Yeah but he ain‚Äôt got one h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                              tweet  \\\n",
       "0             17461978  [RT @CarnivalCruise: üéâ Are you ready to see wh...   \n",
       "1  1297437077403885568                                               None   \n",
       "2             17685258  [RT @realDonaldTrump: THANK YOU #RNC2020! http...   \n",
       "3             15750898  [A family fears they may have been cheated out...   \n",
       "4           1659167666  [RT @VonteThePlug: Yeah but he ain‚Äôt got one h...   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outside-metadata",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11826, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "viral-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df = pd.read_json('profile.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stock-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11826, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-header",
   "metadata": {},
   "source": [
    "Preprocess the tweets before training the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "modular-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_tweets_to_doc(row):\n",
    "    if row is not None:\n",
    "        return ' '.join(row)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "upset-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_doc = tweet_df.copy()\n",
    "# tweets_doc.loc[:,'tweet'] = tweet_df['tweet'].apply(change_tweets_to_doc)\n",
    "# tweets_doc.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-pathology",
   "metadata": {},
   "source": [
    "Taking the first 4 users as an example: We explode the tweets to have one tweet per row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "muslim-geography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(688, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets_doc = tweet_df[:5].explode('tweet').dropna()\n",
    "all_tweets_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alpine-wallace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT @CarnivalCruise: üéâ Are you ready to see wha...\n",
       "0    Who has time for receipts? Not me. @epson rece...\n",
       "0    Steady wants to encourage you to invest in you...\n",
       "0    Good one, @rishid. But let‚Äôs see if y'all can ...\n",
       "0                                 #lsunationalchamps\\n\n",
       "                           ...                        \n",
       "4    When you locking the doors at 10 and a custome...\n",
       "4    Album Out Now GO GET IT üî•üî•üí™üèæ https://t.co/7X4e...\n",
       "4    I make hits, you fuck niggas gon learn that th...\n",
       "4    I‚Äôm the illest up &amp; Coming outta Carolina ...\n",
       "4    Treat me like a king baby talk to me nice ü•∞üòà h...\n",
       "Name: tweet, Length: 688, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets_doc.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "painted-bristol",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885caa604172444aaef69b7edf668184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing using gensim library simple_preprocess function\n",
    "\n",
    "simple_tokenized = [simple_preprocess(sent) for sent in tqdm(all_tweets_doc.tweet)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-thanks",
   "metadata": {},
   "source": [
    "We now have list of tokenized words from the 4 users, we will train the w2v model on them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-payment",
   "metadata": {},
   "source": [
    "Training word2vec model on our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "russian-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_model = Word2Vec(sentences=simple_tokenized, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-secretary",
   "metadata": {},
   "source": [
    "Saving the model's word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amino-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_word_vectors = quick_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-webmaster",
   "metadata": {},
   "source": [
    "To use the word2vec model in calculating similarity between tweets, we apply the following steps:\n",
    "\n",
    "1) For each tokenized sentence in the corpus, we get its w2v matrix\n",
    "\n",
    "2) We then calculate their mean to have only one vector for each sentence\n",
    "\n",
    "3) We concatenate vectors of sentences of each user in a matrix (As an output we will have a matrix of average vectors per sentences for each user)\n",
    "\n",
    "4) We then calculate the cosine similarity for each user's matrix and take the output number to be the similarity feature for supervised learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "normal-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply here on the 1st four users\n",
    "\n",
    "similarity = []\n",
    "for user_sent in tweet_df.tweet[:5]:\n",
    "    w2v = []\n",
    "    # Ignoring users where tweet = None\n",
    "    if user_sent is not None:\n",
    "        # We take each user sentence, apply preprocessing to it (tokenize it), then if the output list isn't empty,\n",
    "        # get the w2v representation of the words of the sentence, then take their mean and append them to the w2v array\n",
    "        # For each user we will have a w2v array created, after all the users sentences are done, we calculate the \n",
    "        # cosine similarity between the w2v vectors and append the value to the similarity list, which will be later on\n",
    "        # appended to the dataset as a feature in the supervised learning model\n",
    "        for sentence in user_sent:\n",
    "            tokenized = simple_preprocess(sentence)\n",
    "            if len(tokenized) > 0:\n",
    "                mean_w2v = np.mean(quick_word_vectors[tokenized],axis=0)\n",
    "                w2v.append(mean_w2v)\n",
    "    \n",
    "        sim = cosine_similarity(w2v)\n",
    "        np.fill_diagonal(sim,0)\n",
    "        similarity.append(sim.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "simple-group",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rolled-peripheral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99187434, 0.9942174, 0.99466115, 0.9878583]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-lesbian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
